{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks: Maths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for the mathematical details of the neural network. Here, mathematical details about forward and backward propagation will be given."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notations used are given in the [file](NN/Notations.md)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $X\\in \\mathbb{R}^{n_x\\times m}$ is the input matrix, there are L layers in the network, and $W^{[l]}\\in \\mathbb{R}^{n^{[l]}\\times n^{[l-1]}}$ and $b^{[l]}\\in \\mathbb{R}^{n^{[l]}\\times 1}$ are the weight and bias matrices for the $l^{th}$ layer. Then, the output of the $l^{th}$ layer is given by"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}\\\\\n",
    "A^{[l]} = g^{[l]}(Z^{[l]})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $g^{[l]}$ is the activation function for the $l^{th}$ layer. The shapes of the $Z$ and $A$'s are - $Z^{[l]}\\in \\mathbb{R}^{n^{[l]}\\times m}$ and $A^{[l]}\\in \\mathbb{R}^{n^{[l]}\\times m}$. This is the forward propagation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note that $A^{[0]} = X$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example of Forward Propagation: Shapes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the following parameters as an example:\n",
    "\n",
    "- $n_x = 10$\n",
    "- $n_y = 3$\n",
    "- $m = 100$\n",
    "- $L = 3$\n",
    "- $n^{[0]} =10, n^{[1]} = 8, n^{[2]} = 5, n^{[3]} = 3$ (Note that $n^{[0]} = n_x$ and $n^{[L]} = n_y$)\n",
    "\n",
    "Then, the shapes of the matrices should be:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W^{[1]} &\\in \\mathbb{R}^{8\\times 10}\\\\\n",
    "b^{[1]} &\\in \\mathbb{R}^{8\\times 1}\\\\\n",
    "W^{[2]} &\\in \\mathbb{R}^{5\\times 8}\\\\\n",
    "b^{[2]} &\\in \\mathbb{R}^{5\\times 1}\\\\\n",
    "W^{[3]} &\\in \\mathbb{R}^{3\\times 5}\\\\\n",
    "b^{[3]} &\\in \\mathbb{R}^{3\\times 1}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the outputs should have the shape:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z^{[1]} &\\in \\mathbb{R}^{8\\times 100}\\\\\n",
    "A^{[1]} &\\in \\mathbb{R}^{8\\times 100}\\\\\n",
    "Z^{[2]} &\\in \\mathbb{R}^{5\\times 100}\\\\\n",
    "A^{[2]} &\\in \\mathbb{R}^{5\\times 100}\\\\\n",
    "Z^{[3]} &\\in \\mathbb{R}^{3\\times 100}\\\\\n",
    "A^{[3]} &\\in \\mathbb{R}^{3\\times 100}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the shape of $Z^{[1]}$, $Z^{[2]}$ and $Z^{[3]}$ from the given equation to see this is indeed the case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Z^{[1]}$ gives\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z^{[1]} &= W^{[1]}A^{[0]} + b^{[1]}\\\\\n",
    "&= (8\\times 10)(10\\times 100) + (8\\times 1)\\\\\n",
    "&= (8\\times 100) + (8\\times 1)\\\\\n",
    "&= (8\\times 100)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that numpy broadcasting is used here, that is, in numpy the bias with shape $(8\\times 1)$ is added to the matrix with shape $(8\\times 100)$ by braodcasting the bias to the shape $(8\\times 100)$. See the numpy broadcasting [documentation](https://numpy.org/doc/stable/user/basics.broadcasting.html) for more details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $Z^{[1]}$ has the shape $(8\\times 100)$ as expected. Let's do the same for $Z^{[2]}$ and $Z^{[3]}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "Z^{[2]} &= W^{[2]}A^{[1]} + b^{[2]}\\\\\n",
    "&= (5\\times 8)(8\\times 100) + (5\\times 1)\\\\\n",
    "&= (5\\times 100) + (5\\times 1)\\\\\n",
    "&= (5\\times 100)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We see that $Z^{[2]}$ has the shape $(5\\times 100)$ as expected. Let's do the same for $Z^{[3]}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z^{[3]} &= W^{[3]}A^{[2]} + b^{[3]}\\\\\n",
    "&= (3\\times 5)(5\\times 100) + (3\\times 1)\\\\\n",
    "&= (3\\times 100) + (3\\times 1)\\\\\n",
    "&= (3\\times 100)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output $A^{[3]}$ is sigmoid of $Z^{[3]}$, so it has the same shape as $Z^{[3]}$. We see that $A^{[3]}$ has the shape $(3\\times 100)$ as expected, since $Y$ has the shape $(3\\times 100)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Numpy Broadcasting Takes Care of the Bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how numpy broadcasting takes care of the bias. We'll take the following example:\n",
    "\n",
    "- $n^{[1]} = 2$\n",
    "- $n^{[2]} = 3$\n",
    "- $m = 4$\n",
    "- $l = 2$\n",
    "\n",
    "The output of the $2^{nd}$ layer then will be\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z^{[2]} &= W^{[2]}A^{[1]} + b^{[2]}\\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrices involved in the euqation will have shape:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W^{[2]} &\\in \\mathbb{R}^{3\\times 2}\\\\\n",
    "A^{[1]} &\\in \\mathbb{R}^{2\\times 4}\\\\\n",
    "b^{[2]} &\\in \\mathbb{R}^{3\\times 1}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose that $A^{[1]}$ has the following matrix form\n",
    "\n",
    "$$\n",
    "A^{[1]} = \\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} & a_{14}\\\\\n",
    "a_{21} & a_{22} & a_{23} & a_{24}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And, the output $W^{[2]}$ is\n",
    "\n",
    "$$\n",
    "W^{[2]} = \\begin{bmatrix}\n",
    "w_{11} & w_{12}\\\\\n",
    "w_{21} & w_{22}\\\\\n",
    "w_{31} & w_{32}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And, the bias $b^{[2]}$ is\n",
    "\n",
    "$$\n",
    "b^{[2]} = \\begin{bmatrix}\n",
    "b_{1}\\\\\n",
    "b_{2}\\\\\n",
    "b_{3}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's multiply the matrices $W^{[2]}$ and $A^{[1]}$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W^{[2]}A^{[1]} &= \\begin{bmatrix}\n",
    "w_{11} & w_{12}\\\\\n",
    "w_{21} & w_{22}\\\\\n",
    "w_{31} & w_{32}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "a_{11} & a_{12} & a_{13} & a_{14}\\\\\n",
    "a_{21} & a_{22} & a_{23} & a_{24}\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "w_{11}a_{11} + w_{12}a_{21} & w_{11}a_{12} + w_{12}a_{22} & w_{11}a_{13} + w_{12}a_{23} & w_{11}a_{14} + w_{12}a_{24}\\\\\n",
    "w_{21}a_{11} + w_{22}a_{21} & w_{21}a_{12} + w_{22}a_{22} & w_{21}a_{13} + w_{22}a_{23} & w_{21}a_{14} + w_{22}a_{24}\\\\\n",
    "w_{31}a_{11} + w_{32}a_{21} & w_{31}a_{12} + w_{32}a_{22} & w_{31}a_{13} + w_{32}a_{23} & w_{31}a_{14} + w_{32}a_{24}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy broadcasts the bais\n",
    "$$\n",
    "b^{[2]} = \\begin{bmatrix}\n",
    "b_{1}\\\\\n",
    "b_{2}\\\\\n",
    "b_{3}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "as\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "b_{1} & b_{1} & b_{1} & b_{1}\\\\\n",
    "b_{2} & b_{2} & b_{2} & b_{2}\\\\\n",
    "b_{3} & b_{3} & b_{3} & b_{3}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the bias to the matrix multiplication gives\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W^{[2]}A^{[1]} + b^{[2]} &= \\begin{bmatrix}\n",
    "w_{11}a_{11} + w_{12}a_{21} & w_{11}a_{12} + w_{12}a_{22} & w_{11}a_{13} + w_{12}a_{23} & w_{11}a_{14} + w_{12}a_{24}\\\\\n",
    "w_{21}a_{11} + w_{22}a_{21} & w_{21}a_{12} + w_{22}a_{22} & w_{21}a_{13} + w_{22}a_{23} & w_{21}a_{14} + w_{22}a_{24}\\\\\n",
    "w_{31}a_{11} + w_{32}a_{21} & w_{31}a_{12} + w_{32}a_{22} & w_{31}a_{13} + w_{32}a_{23} & w_{31}a_{14} + w_{32}a_{24}\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "b_{1} & b_{1} & b_{1} & b_{1}\\\\\n",
    "b_{2} & b_{2} & b_{2} & b_{2}\\\\\n",
    "b_{3} & b_{3} & b_{3} & b_{3}\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "w_{11}a_{11} + w_{12}a_{21} + b_{1} & w_{11}a_{12} + w_{12}a_{22} + b_{1} & w_{11}a_{13} + w_{12}a_{23} + b_{1} & w_{11}a_{14} + w_{12}a_{24} + b_{1}\\\\\n",
    "w_{21}a_{11} + w_{22}a_{21} + b_{2} & w_{21}a_{12} + w_{22}a_{22} + b_{2} & w_{21}a_{13} + w_{22}a_{23} + b_{2} & w_{21}a_{14} + w_{22}a_{24} + b_{2}\\\\\n",
    "w_{31}a_{11} + w_{32}a_{21} + b_{3} & w_{31}a_{12} + w_{32}a_{22} + b_{3} & w_{31}a_{13} + w_{32}a_{23} + b_{3} & w_{31}a_{14} + w_{32}a_{24} + b_{3}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see if the output is correct, we calulate $\\mathbf{z}_3^{[2]}$ which is defined as the output (before activation) of the $3^{nd}$ nueuron in the $2^{nd}$ layer. Phyically, this should be a row vector of length 4. And this should  the $3^{rd}$ row of the matrix $Z^{[2]}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate it by hand as this is nothing but the output of each samples by the third neuron in the second layer. Now, output of the first sample by the third neuron in the second layer is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_{3}^{[2](1)} &= w_{31}a_{11} + w_{32}a_{21} + b_{3}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Similarly, the output of the second sample by the third neuron in the second layer is\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_{3}^{[2](2)} &= w_{31}a_{12} + w_{32}a_{22} + b_{3}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Similarly, the output of the third sample by the third neuron in the second layer is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_{3}^{[2](3)} &= w_{31}a_{13} + w_{32}a_{23} + b_{3}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finally, the output of the fourth sample by the third neuron in the second layer is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_{3}^{[2](4)} &= w_{31}a_{14} + w_{32}a_{24} + b_{3}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting these in a row vector, we get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z}_3^{[2]} &= \\begin{bmatrix}\n",
    "z_{3}^{[2](1)} & z_{3}^{[2](2)} & z_{3}^{[2](3)} & z_{3}^{[2](4)}\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "w_{31}a_{11} + w_{32}a_{21} + b_{3} & w_{31}a_{12} + w_{32}a_{22} + b_{3} & w_{31}a_{13} + w_{32}a_{23} + b_{3} & w_{31}a_{14} + w_{32}a_{24} + b_{3}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as the $3^{rd}$ row of the matrix $Z^{[2]}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another test, we can calculate $Z^{[2](3)}$ which is the output of the third sample by the second layer. This should be the $3^{rd}$ column of the matrix $Z^{[2]}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating this by hand, we get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z^{[2](3)} &= \\begin{bmatrix}\n",
    "z_{1}^{[2](3)}\\\\\n",
    "z_{2}^{[2](3)}\\\\\n",
    "z_{3}^{[2](3)}\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\begin{bmatrix}\n",
    "w_{11}a_{13} + w_{12}a_{23} + b_{1}\\\\\n",
    "w_{21}a_{13} + w_{22}a_{23} + b_{2}\\\\\n",
    "w_{31}a_{13} + w_{32}a_{23} + b_{3}\n",
    "\\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as the $3^{rd}$ column of the matrix $Z^{[2]}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the forward propagation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General equation of backpropagation is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} &= \\delta^{[l]} \\odot \\mathbf{A}^{[l-1] T}\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}} &= \\sum_{i=1}^{m} \\delta^{[l](i)}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta^{[l]} &= (W^{[l+1] T} \\odot \\delta^{[l+1]})  g^{[l]'}(Z^{[l]})\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $g^{[l]'}(Z^{[l]})$ is the derivative of the activation function of the $l^{th}$ layer and $\\odot$ is the dot product."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of $\\delta^{[l]}$ is calculated using the value of $\\delta^{[l+1]}$ which is calculated using the value of $\\delta^{[l+2]}$ and so on. This is called the backpropagation. The value of $\\delta^{[L]}$ can be calculated using the following equation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta^{[L]} &= \\frac{1}{m} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}^{[L]}}  g^{[L]'}(Z^{[L]})\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}^{[L]}}$ is the derivative of the cost function with respect to the output of the $L^{th}$ layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving the Backpropagation Equations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations can be derived by using chain rule. Let's define a neural network with 3 layers. The forward propagation is defined as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A^{[0]} &= X\\\\\n",
    "Z^{[1]} &= W^{[1]} A^{[0]} + \\mathbf{b^{[1]}}\\\\\n",
    "A^{[1]} &= g^{[1]}(Z^{[1]})\\\\\n",
    "Z^{[2]} &= W^{[2]} A^{[1]} + \\mathbf{b^{[2]}}\\\\\n",
    "A^{[2]} &= g^{[2]}(Z^{[2]})\\\\\n",
    "Z^{[3]} &= W^{[3]} A^{[2]} + \\mathbf{b^{[3]}}\\\\\n",
    "A^{[3]} &= g^{[3]}(Z^{[3]})\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $g^{[1]}$, $g^{[2]}$ and $g^{[3]}$ are the activation functions of the first, second and third layers respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use mean squared error as the cost function. This gives\n",
    "\n",
    "$$\n",
    "\\mathcal{J} = \\frac{1}{2m}\\sum \\left(A^{[3]} - Y \\right)^2\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, differentaiation at the last layer is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[3]}} = \\frac{\\partial{\\mathcal{J}}}{\\partial A^{[3]}} \\frac{\\partial A^{[3]}}{\\partial Z^{[3]}} \\frac{\\partial Z^{[3]}}{\\partial W^{[3]}}\n",
    "$$\n",
    "For the last layer, we can solve this:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial A^{[3]}} &= \\frac{\\partial}{\\partial A^{[3]}} \\frac{1}{2m}\\sum \\left(A^{[3]} - Y \\right)^2\\\\\n",
    "&= \\frac{1}{m} \\left(A^{[3]} - Y \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial A^{[3]}}{\\partial Z^{[3]}} &= \\frac{\\partial}{\\partial Z^{[3]}} g^{[3]}(Z^{[3]})\\\\\n",
    "&= g^{[3]'}(Z^{[3]})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial Z^{[3]}}{\\partial W^{[3]}} &= \\frac{\\partial}{\\partial W^{[3]}} \\left(W^{[3]} A^{[2]} + \\mathbf{b^{[3]}} \\right)\\\\\n",
    "&= A^{[2]}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all these together, we get\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[3]}} = \\frac{1}{m} \\left(A^{[3]} - Y \\right) g^{[3]'}(Z^{[3]}) A^{[2] T}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two terms are the same as the equation for $\\delta^{[3]}$. Next, we will calculate the derivative of the cost function with respect to the bias of the third layer. This is the same as above except that we replace $W^{[3]}$ with $\\mathbf{b^{[3]}}$. The final equation is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial \\mathbf{b^{[3]}}} = \\frac{1}{m} \\sum \\left(A^{[3]} - Y \\right) g^{[3]'}(Z^{[3]})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's proceed to the second layer. The equation is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[2]}} = \\frac{\\partial{\\mathcal{J}}}{\\partial A^{[3]}} \\frac{\\partial A^{[3]}}{\\partial Z^{[3]}} \\frac{\\partial Z^{[3]}}{\\partial A^{[2]}} \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} \\frac{\\partial Z^{[2]}}{\\partial W^{[2]}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two terms are the same as the equation for $\\delta^{[3]}$. Using this and rewriting gives\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[2]}} =  W^{[3]T} \\odot \\delta^{[3]} g^{[2]'}(Z^{[2]}) \\odot A^{[1]T}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing in termes of the notation of $\\delta^{[l]}$ gives\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[2]}} = \\delta^{[2]} \\odot A^{[1] T}\n",
    "$$\n",
    "with\n",
    "\n",
    "$$\n",
    "\\delta^{[2]} = W^{[3] T} \\odot \\delta^{[3]} g^{[2]'}(Z^{[2]})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we are following the general formula given. Let's calculate the derivative of the first layer. The equation is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[1]}} = \\frac{\\partial{\\mathcal{J}}}{\\partial A^{[3]}} \\frac{\\partial A^{[3]}}{\\partial Z^{[3]}} \\frac{\\partial Z^{[3]}}{\\partial A^{[2]}} \\frac{\\partial A^{[2]}}{\\partial Z^{[2]}} \\frac{\\partial Z^{[2]}}{\\partial A^{[1]}} \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} \\frac{\\partial Z^{[1]}}{\\partial W^{[1]}}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be rewritten as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[1]}} =  W^{[2] T} \\odot \\delta^{[2]} g^{[1]'}(Z^{[1]}) \\odot A^{[0] T}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is indeed what we needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation: Shapes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll take the same example as above, namely:\n",
    "\n",
    "- $n_x = 10$\n",
    "- $n_y = 3$\n",
    "- $m = 100$\n",
    "- $L = 3$\n",
    "- $n^{[0]} =10, n^{[1]} = 8, n^{[2]} = 5, n^{[3]} = 3$ (Note that $n^{[0]} = n_x$ and $n^{[L]} = n_y$)\n",
    "\n",
    "Then, the shapes of the matrices should be:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W^{[1]} &\\in \\mathbb{R}^{8\\times 10}\\\\\n",
    "b^{[1]} &\\in \\mathbb{R}^{8\\times 1}\\\\\n",
    "W^{[2]} &\\in \\mathbb{R}^{5\\times 8}\\\\\n",
    "b^{[2]} &\\in \\mathbb{R}^{5\\times 1}\\\\\n",
    "W^{[3]} &\\in \\mathbb{R}^{3\\times 5}\\\\\n",
    "b^{[3]} &\\in \\mathbb{R}^{3\\times 1}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the outputs should have the shape:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Z^{[1]} &\\in \\mathbb{R}^{8\\times 100}\\\\\n",
    "A^{[1]} &\\in \\mathbb{R}^{8\\times 100}\\\\\n",
    "Z^{[2]} &\\in \\mathbb{R}^{5\\times 100}\\\\\n",
    "A^{[2]} &\\in \\mathbb{R}^{5\\times 100}\\\\\n",
    "Z^{[3]} &\\in \\mathbb{R}^{3\\times 100}\\\\\n",
    "A^{[3]} &\\in \\mathbb{R}^{3\\times 100}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, the derivatives should have the shape as the parameters:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[1]}} &\\in \\mathbb{R}^{8\\times 10}\\\\\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial b^{[1]}} &\\in \\mathbb{R}^{8\\times 1}\\\\\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[2]}} &\\in \\mathbb{R}^{5\\times 8}\\\\\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial b^{[2]}} &\\in \\mathbb{R}^{5\\times 1}\\\\\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[3]}} &\\in \\mathbb{R}^{3\\times 5}\\\\\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial b^{[3]}} &\\in \\mathbb{R}^{3\\times 1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from the last layer. We have\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[3]}} = \\frac{1}{m} \\left(A^{[3]} - Y \\right) g^{[3]'}(Z^{[3]}) A^{[2] T}\n",
    "$$\n",
    "\n",
    "Writing the shapes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[3]}} = (3, 100).(3, 100)\\odot (100, 5) = (3, 5)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial \\mathbf{b^{[3]}}} = \\sum \\left(A^{[3]} - Y \\right) g^{[3]'}(Z^{[3]})\\\\\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial \\mathbf{b^{[3]}}} = \\sum (3, 100) = (3,1)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the second layer:\n",
    "$$\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[2]}} =  W^{[3]T} \\odot \\delta^{[3]} g^{[2]'}(Z^{[2]}) \\odot A^{[1] T}\\\\\n",
    "\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[2]}} = (5,3)\\odot (3,100)\\cdot (5, 100)\\odot (100, 8) =(5, 8)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can show that $$\\frac{\\partial{\\mathcal{J}}}{\\partial W^{[2]}}$$ follows the shape given by the above table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the final equation of back-propagation is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W^{[l]}} &= \\delta^{[l]} \\mathbf{A}^{[l-1] T}\\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b^{[l]}} &= \\sum_{i=1}^{m} \\delta^{[l](i)}\\\\\n",
    "\\delta^{[l]} &= (W^{[l+1] T} \\odot \\delta^{[l+1]})  g^{[l]'}(Z^{[l]})\\\\\n",
    "\\delta^{[L]} &= \\frac{1}{m} \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{A}^{[L]}}  g^{[L]'}(Z^{[L]})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network: Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 5\n",
    "nx = 10\n",
    "ny = 3\n",
    "m = 10\n",
    "nuerons = [20, 10, 10, 5, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_num_to_str(layer_num):\n",
    "    return f'layer_{layer_num}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights(nuerons, nx):\n",
    "    weights = {}\n",
    "    for i in range(len(nuerons)):\n",
    "        if i == 0:\n",
    "            weights[layer_num_to_str(i)] = np.random.randn(nuerons[i], nx)\n",
    "        else:\n",
    "            weights[layer_num_to_str(i)] = np.random.randn(nuerons[i], nuerons[i-1])\n",
    "    return weights\n",
    "\n",
    "def create_biases(nuerons):\n",
    "    biases = {}\n",
    "    for i in range(len(nuerons)):\n",
    "        biases[layer_num_to_str(i)] = np.zeros((nuerons[i], 1))\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = create_weights(nuerons, nx)\n",
    "biases = create_biases(nuerons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['layer_0', 'layer_1', 'layer_2', 'layer_3', 'layer_4']),\n",
       " dict_keys(['layer_0', 'layer_1', 'layer_2', 'layer_3', 'layer_4']))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.keys(), biases.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b):\n",
    "    Z = linear_forward(A_prev, W, b)\n",
    "    A = sigmoid(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(nx, m)\n",
    "\n",
    "def forward_propagation(X, weights, biases):\n",
    "    A = X\n",
    "    for i in range(num_layers):\n",
    "        A_prev = A\n",
    "        A = linear_activation_forward(A_prev, weights[layer_num_to_str(i)], biases[layer_num_to_str(i)])\n",
    "    return A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward_propagation` is the main function that implements the forward propagation. It takes the input data `X` and the parameters `parameters` as input and returns the output `A`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = forward_propagation(X, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert output.shape == (ny, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.39331398, 0.38826564, 0.38970083, 0.38939174, 0.39196332,\n",
       "        0.38967909, 0.3873361 , 0.38904605, 0.3931707 , 0.38847667],\n",
       "       [0.51676666, 0.51432102, 0.51292546, 0.51546524, 0.52068796,\n",
       "        0.51693211, 0.51944096, 0.51348509, 0.51354485, 0.51644627],\n",
       "       [0.06149044, 0.05909642, 0.05895807, 0.05820592, 0.06068014,\n",
       "        0.05798942, 0.06320216, 0.05879738, 0.06084828, 0.06036028]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2efee1efa502125d01e6b4768ba06d9453d29f3642bfd14ad5d4a769de82e88c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
