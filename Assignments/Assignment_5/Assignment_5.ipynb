{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "X_train = loadmat(os.path.join(DATA_DIR, \"train_images.mat\"))[\"train_images\"]\n",
    "X_test = loadmat(os.path.join(DATA_DIR, \"test_images.mat\"))[\"test_images\"]\n",
    "y_train = loadmat(os.path.join(DATA_DIR, \"train_labels.mat\"))[\"train_labels\"]\n",
    "y_test = loadmat(os.path.join(DATA_DIR, \"test_labels.mat\"))[\"test_labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y, n_classes):\n",
    "    \"\"\"\n",
    "    Converts a vector of labels into a one-hot matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        An array of shape (m, ) that contains labels for X. Each value in y\n",
    "        should be an integer in the range [0, n_classes).\n",
    "\n",
    "    n_classes : int\n",
    "        The number of classes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    one_hot : array_like\n",
    "        An array of shape (m, n_classes) where each row is a one-hot vector.\n",
    "    \"\"\"\n",
    "    if len(y.shape) > 1:\n",
    "        raise ValueError(\"y should be a vector\")\n",
    "    m = y.shape[0]\n",
    "    one_hot = np.zeros((n_classes, m))\n",
    "    for i in range(m):\n",
    "        one_hot[y[i], i] = 1\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 28, 28, 1), (1000, 28, 28, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the data compatible with the model\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "X_train = X_train.reshape(-1, 28,28,1)\n",
    "X_test = X_test.reshape(-1, 28,28,1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "X_train = X_train / X_train.max() - 0.5\n",
    "X_test = X_test / X_test.max() - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 10), (1000, 10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding the labels\n",
    "y_train = np.squeeze(y_train)\n",
    "y_test = np.squeeze(y_test)\n",
    "y_train = one_hot(y_train, 10)\n",
    "y_test = one_hot(y_test, 10)\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    #(â‰ˆ 1 line)\n",
    "    # X_pad = None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    X_pad = np.pad(X, ((0,0), (pad, pad), (pad, pad), (0,0)), mode=\"constant\", constant_values = (0,0))\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return X_pad\n",
    "\n",
    "# GRADED FUNCTION: conv_single_step\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    s = np.multiply(a_slice_prev,W)\n",
    "    Z = np.sum(s)\n",
    "    b = np.squeeze(b)\n",
    "    Z = Z + b\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    return Z\n",
    "\n",
    "# GRADED FUNCTION: conv_forward\n",
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, \n",
    "        numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE STARTS HERE\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    n_H = int((n_H_prev + 2*pad - f)/stride) + 1\n",
    "    n_W = int((n_W_prev + 2*pad - f)/stride) + 1\n",
    "    \n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        for h in range(n_H):\n",
    "            vert_start = stride * h \n",
    "            vert_end = vert_start  + f\n",
    "            \n",
    "            for w in range(n_W): \n",
    "                horiz_start = stride * w\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range(n_C):\n",
    "                    \n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    weights = W[:, :, :, c]\n",
    "                    biases  = b[:, :, :, c]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n",
    "                    \n",
    "    cache = (A_prev, W, b, hparameters)             \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"    \n",
    "    \n",
    "        \n",
    "    # YOUR CODE STARTS HERE\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    dA_prev = np.zeros(A_prev.shape)                          \n",
    "    dW = np.zeros(W.shape)\n",
    "    db = np.zeros(b.shape) # b.shape = [1,1,1,n_C]\n",
    "    \n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = stride * h \n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = stride * w\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpadded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        if pad:\n",
    "            dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "        else:\n",
    "            dA_prev[i, :, :, :] = da_prev_pad\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "# GRADED FUNCTION: pool_forward\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    for i in range(m):\n",
    "        a_prev_slice = A_prev[i]\n",
    "        for h in range(n_H):\n",
    "            vert_start = stride * h \n",
    "            vert_end = vert_start + f\n",
    "            \n",
    "            for w in range(n_W): \n",
    "                \n",
    "                horiz_start = stride * w\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range (n_C):\n",
    "                    \n",
    "                    a_slice_prev = a_prev_slice[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                    \n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_slice_prev)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_slice_prev)\n",
    "                    else:\n",
    "                        print(mode+ \"-type pooling layer NOT Defined\")  \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    #assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"    \n",
    "    # (â‰ˆ1 line)\n",
    "    # mask = None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    mask = (x == np.max(x))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return mask\n",
    "\n",
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"    \n",
    "\n",
    "    (n_H, n_W) = shape\n",
    "    average = np.prod(shape)\n",
    "    a = (dz/average)*np.ones(shape)\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m): # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (â‰ˆ1 line)\n",
    "        a_prev = A_prev[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "        \n",
    "                    # Find the corners of the current \"slice\" (â‰ˆ4 lines)\n",
    "                    vert_start  = h * stride\n",
    "                    vert_end    = h * stride + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end   = w * stride + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (â‰ˆ1 line)\n",
    "                        a_prev_slice = a_prev[ vert_start:vert_end, horiz_start:horiz_end, c ]\n",
    "                        \n",
    "                        # Create the mask from a_prev_slice (â‰ˆ1 line)\n",
    "                        mask = create_mask_from_window( a_prev_slice )\n",
    "\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (â‰ˆ1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value da from dA (â‰ˆ2 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        \n",
    "                        # Define the shape of the filter as fxf (â‰ˆ1 line)\n",
    "                        shape = (f,f)\n",
    "\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (â‰ˆ1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev\n",
    "\n",
    "def dense_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A_prev\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    #Z = np.dot(W, A_prev) + b\n",
    "    Z = np.dot(W, A_prev.T) + b\n",
    "    # assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (A_prev, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "def dense_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    # assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def initialize_W_b(neurons, output_shape):\n",
    "    W = np.random.randn(neurons, output_shape)\n",
    "    b = np.zeros((neurons, 1))\n",
    "    return W, b\n",
    "\n",
    "def initialize_kernel(kernel_size, input_channels, output_channels):\n",
    "    kernel = np.random.randn(kernel_size, kernel_size, input_channels, output_channels) * np.sqrt(\n",
    "        2 / (kernel_size * kernel_size * input_channels)\n",
    "    )\n",
    "    bias = np.zeros((1, 1, 1, output_channels))\n",
    "    return kernel, bias\n",
    "\n",
    "def softmax_forward(z):\n",
    "    z -= np.max(\n",
    "        z, axis=0, keepdims=True\n",
    "    )  # axis=0 means coloumn z is the shape of (n_l, batch_size), axis=0 means the max value of each column b/c we are giving input as (n_l, batch_size)\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True), None\n",
    "\n",
    "def softmax_backward(z):\n",
    "    # we have calculated the dA/dz in the loss_prime itself,\n",
    "    # that returns (dJ/dA)*(dA/dz) itself so no need to take the derivative of activation here\n",
    "    return z\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    m_samples = y_pred.shape[-1]\n",
    "\n",
    "    cost = -np.sum(y_true * np.log(y_pred + 1e-10))   # shape = (batch_size,)\n",
    "\n",
    "    cost = cost / m_samples  \n",
    "\n",
    "\n",
    "    return cost\n",
    "\n",
    "def loss_backward(y_true, y_pred):\n",
    "    m_samples = y_pred.shape[-1]\n",
    "\n",
    "    # this is little bit different from the else loss_prime,\n",
    "    # this return the (dJ/dA)*(dA/dz) so we don't need to find the derivative of sofmax_prime \n",
    "    cost_prime = (y_pred - y_true) / m_samples\n",
    "\n",
    "    return cost_prime\n",
    "\n",
    "\n",
    "def flatten_forward(A_prev):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for a flatten layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- activations from the previous layer (or input data): (size of previous layer, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    A -- flatten output of the activation function, also called \"post-activation\" value \n",
    "    cache -- a python dictionary containing \"A_prev\"; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    # (â‰ˆ1 line)\n",
    "    # A = None\n",
    "    # YOUR CODE STARTS HERE\n",
    "    A = A_prev.reshape(A_prev.shape[0], -1)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Store input shape in \"cache\" for the backprop\n",
    "    cache = A_prev.shape\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def flatten_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a flatten layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of the cost with respect to the flatten output (of the current layer l)\n",
    "    cache -- cache of values needed for the flatten_backward(), output of flatten_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev_shape) = cache\n",
    "    \n",
    "    # Reshape dA to A_prev_shape\n",
    "    dA_prev = dA.reshape(A_prev_shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels_shape = (3,3, 1, 9)\n",
    "b_shape = (1, 1, 1, 9)\n",
    "kernel_W = np.random.randn(*kernels_shape)\n",
    "kernel_b = np.random.randn(*b_shape)\n",
    "W, b = initialize_W_b(10, 13*13*9)\n",
    "\n",
    "\n",
    "\n",
    "hparameters = {\"pad\" : 0,\n",
    "                \"stride\": 1}\n",
    "\n",
    "max_pool_hparameters = {\"stride\" : 2,\n",
    "                        \"f\": 2}\n",
    "mode = \"max\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(X_train, y_train, lr, parameters):\n",
    "    #Forward Propagation\n",
    "    kernel_W = parameters[\"kernel_W\"]\n",
    "    kernel_b = parameters[\"kernel_b\"]\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "\n",
    "    A_prev = X_train\n",
    "    A_prev, cache_conv = conv_forward(A_prev, kernel_W, kernel_b, hparameters)\n",
    "    A_prev, cache_pool = pool_forward(A_prev, max_pool_hparameters, mode)\n",
    "    A_prev, cache_flatten = flatten_forward(A_prev)\n",
    "    A_prev, cache_dense = dense_forward(A_prev, W, b)\n",
    "    A_prev, cache_softmax = softmax_forward(A_prev)\n",
    "\n",
    "    # Backward pass\n",
    "    dA_prev = loss_backward(y_train, A_prev)\n",
    "    dA_prev = softmax_backward(dA_prev)\n",
    "    dA_prev, dW, db = dense_backward(dA_prev, cache_dense)\n",
    "    W -= lr * dW\n",
    "    b -= lr * db\n",
    "    dA_prev = flatten_backward(dA_prev, cache_flatten)\n",
    "    dA_prev = pool_backward(dA_prev, cache_pool)\n",
    "    dA_prev, dW, db = conv_backward(dA_prev, cache_conv)\n",
    "    kernel_W -= lr * dW\n",
    "    kernel_b -= lr * db\n",
    "\n",
    "    params = {\n",
    "        \"kernel_W\": kernel_W,\n",
    "        \"kernel_b\": kernel_b,\n",
    "        \"W\": W,\n",
    "        \"b\": b\n",
    "    }\n",
    "    loss_val = loss(y_train, A_prev)\n",
    "    accuracy_val = accuracy(y_train, A_prev)\n",
    "    return params, loss_val, accuracy_val\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_pred = np.argmax(y_pred, axis=0)\n",
    "    y_true = np.argmax(y_true, axis=0)\n",
    "    return np.sum(y_pred == y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample(X, y, n):\n",
    "    index = np.random.choice(len(X), n, replace=False)\n",
    "    return X[index], y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 19.0564, Accuracy: 0.1683\n",
      "Epoch: 2, Loss: 18.3659, Accuracy: 0.1485\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"kernel_W\": kernel_W,\n",
    "    \"kernel_b\": kernel_b,\n",
    "    \"W\": W,\n",
    "    \"b\": b\n",
    "}\n",
    "epochs = 2\n",
    "losses = []\n",
    "X, y = random_sample(X_train, y_train, 101)\n",
    "for i in range(epochs):\n",
    "    # parameters, loss_val, acc = train_one(X_train, y_train, 0.01, parameters)\n",
    "    parameters, loss_val, acc = train_one(X, y.T, 0.01, parameters)\n",
    "    losses.append(loss_val)\n",
    "    print(f\"Epoch: {i+1}, Loss: {loss_val:.4f}, Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    kernel_W = parameters[\"kernel_W\"]\n",
    "    kernel_b = parameters[\"kernel_b\"]\n",
    "    W = parameters[\"W\"]\n",
    "    b = parameters[\"b\"]\n",
    "\n",
    "    A_prev = X\n",
    "    A_prev, cache_conv = conv_forward(A_prev, kernel_W, kernel_b, hparameters)\n",
    "    A_prev, cache_pool = pool_forward(A_prev, max_pool_hparameters, mode)\n",
    "    A_prev, cache_flatten = flatten_forward(A_prev)\n",
    "    A_prev, cache_dense = dense_forward(A_prev, W, b)\n",
    "    A_prev, cache_softmax = softmax_forward(A_prev)\n",
    "    return A_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = accuracy(y_train.T, predict(X_train, parameters))\n",
    "test_acc = accuracy(y_test.T, predict(X_test, parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.1090, Test Accuracy: 0.0930\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "qO8ng",
   "launcher_item_id": "7XDi8"
  },
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "a4a1b8dfbfcf197657bbe13300384e076b1914ebd98f1200feef09f71d03b223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
