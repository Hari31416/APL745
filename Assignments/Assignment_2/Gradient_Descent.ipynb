{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A General Gradient Descent Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notations which we will follow throught the assignment (and future assignments too) are:\n",
    "\n",
    "1. A scalar is denoted by regular lowercase letter, eg. $x, y, t, w$.\n",
    "2. A vector, which is denoted by boldface lowercase letter, eg. $\\mathbf{x}, \\mathbf{y}, \\mathbf{w}$.\n",
    "3. A vector, if not mentioned otherwise, will be a column vector, like\n",
    "   $$\n",
    "   \\mathbf{x} = \\begin{bmatrix}\n",
    "   x_1 \\\\\n",
    "   x_2 \\\\\n",
    "   \\vdots \\\\\n",
    "   x_m \\\\\n",
    "   \\end{bmatrix}\n",
    "   $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. A matrix is denoted by a regular uppercase letter, eg. $X, W$.\n",
    "5. Number of training examples is denoted by $m$.\n",
    "6. Number of features is denoted by $n$.\n",
    "7. $X \\in \\mathbb{R}^{m\\times n}$ is the input matrix.\n",
    "8. $X^{(i)} \\in \\mathbb{R}^{n}$ is the $i^{th}$ example in the input matrix.\n",
    "9. $\\mathbf{w} \\in \\mathbb{R}^{n}$ denotes the coeficient vector and $b$ is the intercept.\n",
    "10. $\\hat{y}$ is the predicted value.\n",
    "11. The hypothesis function is denoted by $h(\\theta)$ where $\\theta \\in \\mathbb{R}^{n+1}$ includes $\\mathbf{w}$ and $b$.\n",
    "12. The loss/cost function for the whole data is denoted by $J(X, \\mathbf{y}, \\mathbf{w}, b)$ or $J(\\hat{y}, \\mathbf{y})$.\n",
    "13. $\\partial J_{w_{i}}$ denotes the partial derivative of $J$ with respect to $w_{i}$, namely, $\\frac{\\partial J}{\\partial w_{i}}$. Same goes for $b$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalism of the Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the input vector $X \\in \\mathbb{R}^{n\\times m}$ with $n$ number of features and $m$ number of examples. The hypothesis function will be:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{w}) = \\mathbf{w}^T X\n",
    "$$\n",
    "where $\\mathbf{w} \\in \\mathbb{R}^{n}$ or $\\mathbf{w} \\in \\mathbb{R}^{n+1}$ (if bias term is included) is a column vector. \n",
    "\n",
    "We can see that the hypothesis function $h(\\mathbf{w}) \\in \\mathbb{R}^m$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J(\\hat{y}, \\mathbf{y}) &= \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2\\\\\n",
    "    &= \\frac{1}{2m} \\sum_{i=1}^{m} (h(\\mathbf{w})^{(i)} - y^{(i)})^2\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient given by:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\mathbf{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) \\mathbf{x}^{(i)}\\\\\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent update rule is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{w} &:= \\mathbf{w} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{w}}\\\\\n",
    "&:=\\mathbf{w} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)}) \\mathbf{x}^{(i)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have included $b$ in $\\mathbf{w}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of gradient descent can be found in the module `GD.py`. The code is structured as follow:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To increase the reusability of code, I've used inheritance, where one class inherits some of the properties from the parent class. This is done because there are a lot of similarity among linear gradient descent, batch gradient descent, stochastic gradient descent as well as their logistic regression counterparts. Using inheritence means I will be able to save hundreds of lines of code and make the code more readable as well as more maintainable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `GradientDescent` class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the base class for all gradient descent algorithms. Any gradient descent algorithm, inheriting from this base class get some attributes and methods for free.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class is instantiated with the following parameters:\n",
    "\n",
    "* `fit_intercept`: If `True`, the bias term $b$ is included in $\\mathbf{w}$.\n",
    "* `tol`: The tolerance for the stopping criterion. The algorithm stops when the change in the loss function is less than `tol`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `fit_intercept` is set to `True` and `tol` is set to `None`, which means that the stopping criterion is not used."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from just being the base class for gradient descent, the class can be used for a multilinear regression. The class has a `fit` method which takes the input matrix $X$, the target variable $y$ along with `learning_rate`, `epochs` and some other parameters. The `fit` method fits the model. The learned parameters are stored in the `self.weights` attribute. The `predict` method takes the input matrix $X$ and returns the predicted values. These two are the main methods of the class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `callback`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method accepts a `callable` which takes in input parameters, `self`, which is the model in question, `weights`, which is the weight of the model at the current iteration and `epoch`, which is the current epoch. This is useful for plotting the loss function as well as the weights as the model is being trained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `BatchGradientDescent` class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class inherits from the `GradientDescent` class. The `fit` method is the same as the `GradientDescent` class. The `predict` method is also the same. The only difference is that the `fit` method uses the batch gradient descent algorithm. and hence accepts a parameter `batch_size` which controls what batch size to use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Stochastic Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent is nothing but a batch gradient descent with batch size of 1. So, SGD can be implemented by setting the `batch_size` to 1 for the `BatchGradientDescent` class. I've not written a separate class for SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GD import BatchGradientDescent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = BatchGradientDescent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gd_logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 14:58:50,681 - gd_logger - WARNING - This is a warning\n"
     ]
    }
   ],
   "source": [
    "logger.warning(\"This is a warning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"This is an info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.debug(\"This is a debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-22 14:49:36,721 - gd_logger - CRITICAL - This is a critical\n"
     ]
    }
   ],
   "source": [
    "logger.critical(\"This is a critical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2efee1efa502125d01e6b4768ba06d9453d29f3642bfd14ad5d4a769de82e88c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
